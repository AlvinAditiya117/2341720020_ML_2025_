{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1XHoa8L9LpBArKrFlMPmLZB66RHNE1UzM","authorship_tag":"ABX9TyPXgtfBHQnl4OPGKsgqHFlZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Tugas 2 Jobsheet 7**"],"metadata":{"id":"A-VehNKfZ7yZ"}},{"cell_type":"code","source":["!pip install annoy faiss-cpu hnswlib\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aK3MXnsT5zp3","executionInfo":{"status":"ok","timestamp":1760029084842,"user_tz":-420,"elapsed":84214,"user":{"displayName":"RyuzaVin17","userId":"02582902285823239711"}},"outputId":"54bfaa1e-d344-4b34-8ea4-318c936beba7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting annoy\n","  Downloading annoy-1.17.3.tar.gz (647 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m645.1/647.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Collecting hnswlib\n","  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n","Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: annoy, hnswlib\n","  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for annoy: filename=annoy-1.17.3-cp312-cp312-linux_x86_64.whl size=551809 sha256=fff786b3290ce2ba7328c927fc99e3da936530cf97ff2a4ec6c83da9d381aad2\n","  Stored in directory: /root/.cache/pip/wheels/db/b9/53/a3b2d1fe1743abadddec6aa541294b24fdbc39d7800bc57311\n","  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp312-cp312-linux_x86_64.whl size=2528145 sha256=f1e45a9af8f7d149e11447ff6991a14586064b9900620d44416603603bf60c23\n","  Stored in directory: /root/.cache/pip/wheels/ac/39/b3/cbd7f9cbb76501d2d5fbc84956e70d0b94e788aac87bda465e\n","Successfully built annoy hnswlib\n","Installing collected packages: annoy, hnswlib, faiss-cpu\n","Successfully installed annoy-1.17.3 faiss-cpu-1.12.0 hnswlib-0.8.0\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"BFqExm_EZ5Qe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760029730340,"user_tz":-420,"elapsed":418882,"user":{"displayName":"RyuzaVin17","userId":"02582902285823239711"}},"outputId":"b49e9ce7-f6ed-4823-9d44-6385b54870d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Membaca CSV...\n","Dropped 0 rows karena NaN. Sisa baris: 955320\n","Dataset: 955320 samples, dim = 9\n","Membangun exact (brute-force) reference dengan sklearn NearestNeighbors...\n","Brute build time: 4.537s, avg query time: 6.0751 ms/query\n","\n","--- ANNOY ---\n","Annoy build time: 91.639s (n_trees=50)\n","Annoy: avg query time = 0.3451 ms/query, recall@10 = 0.9927\n","\n","--- FAISS (CPU) ---\n","Faiss IndexFlatL2 build time: 0.087s\n","Faiss(IndexFlatL2) exact: avg query time = 1.5554 ms/query, recall@10 = 0.9981\n","Faiss IndexIVFFlat build/train time: 0.328s (nlist=100, nprobe=10)\n","Faiss(IndexIVFFlat) approx: avg query time = 0.7234 ms/query, recall@10 = 0.9975\n","\n","--- HNSWLIB ---\n","Error running hnswlib: 'hnswlib.Index' object has no attribute 'get_ef'\n","\n","=== SUMMARY ===\n","[\n","  {\n","    \"name\": \"Annoy\",\n","    \"avg_query_ms\": 0.3451261520385742,\n","    \"recall_at_k\": 0.992699999999999,\n","    \"build_time_s\": 91.63931703567505,\n","    \"params\": {\n","      \"n_trees\": 50,\n","      \"metric\": \"euclidean\"\n","    }\n","  },\n","  {\n","    \"name\": \"Faiss(IndexFlatL2) exact\",\n","    \"avg_query_ms\": 1.5553958415985107,\n","    \"recall_at_k\": 0.9980999999999997,\n","    \"build_time_s\": 0.08664727210998535\n","  },\n","  {\n","    \"name\": \"Faiss(IndexIVFFlat) approx\",\n","    \"avg_query_ms\": 0.7233502864837646,\n","    \"recall_at_k\": 0.9974999999999996,\n","    \"build_time_s\": 0.32810115814208984,\n","    \"params\": {\n","      \"nlist\": 100,\n","      \"nprobe\": 10\n","    }\n","  }\n","]\n","Hasil disimpan ke ann_index_comparison_results.csv\n"]}],"source":["\n","\n","import time\n","import numpy as np\n","import pandas as pd\n","import os\n","import gc\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import NearestNeighbors\n","\n","\n","# -----------------------\n","# Konfigurasi percobaan\n","# -----------------------\n","CSV_PATH = \"/content/drive/MyDrive/Colab Notebooks/Pembelarajan Mesin/Minggu 7/songs_with_attributes_and_lyrics.csv\"   # ganti sesuai path Anda\n","FEATURES = ['danceability', 'energy', 'loudness', 'speechiness',\n","            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","\n","k = 10               # hitung nearest neighbors @k\n","n_queries = 1000     # jumlah query acak (ubah sesuai ukuran dataset / resource)\n","use_subset = None    # jika ingin uji pada subset data set e.g. 50000 -> set int, atau None = pakai semua baris\n","\n","# -----------------------\n","# 1) Load dataset & preprocessing\n","# -----------------------\n","if not os.path.exists(CSV_PATH):\n","    raise FileNotFoundError(f\"File '{CSV_PATH}' tidak ditemukan. Letakkan CSV yang Anda unduh sebagai '{CSV_PATH}'\")\n","\n","print(\"Membaca CSV...\")\n","df = pd.read_csv(CSV_PATH)\n","\n","# Pastikan fitur ada; jika tidak, pilih semua numeric\n","missing = [f for f in FEATURES if f not in df.columns]\n","if missing:\n","    print(\"Kolom yang diminta tidak semua ada. Mengambil semua kolom numerik yang tersedia.\")\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n","    # filter out id/unncessary columns? we'll use numeric_cols\n","    FEATURES = numeric_cols\n","else:\n","    # tetap gunakan FEATURES user-specified\n","    pass\n","\n","# Drop rows with NaN pada fitur yang digunakan\n","df_feat = df[FEATURES].copy()\n","before = len(df_feat)\n","df_feat = df_feat.dropna(axis=0, how='any')\n","after = len(df_feat)\n","print(f\"Dropped {before-after} rows karena NaN. Sisa baris: {after}\")\n","\n","# Optionally take subset (faster testing)\n","if use_subset is not None and use_subset < len(df_feat):\n","    df_feat = df_feat.sample(n=use_subset, random_state=RANDOM_SEED).reset_index(drop=True)\n","\n","X = df_feat.values.astype('float32')  # faiss expects float32\n","n_samples, dim = X.shape\n","print(f\"Dataset: {n_samples} samples, dim = {dim}\")\n","\n","# Standardize\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X).astype('float32')\n","\n","# Build query set (random indices)\n","if n_queries > n_samples:\n","    n_queries = n_samples\n","query_idx = np.random.choice(n_samples, size=n_queries, replace=False)\n","X_queries = X_scaled[query_idx]\n","\n","# For evaluation: compute exact NN using sklearn NearestNeighbors (brute force L2)\n","print(\"Membangun exact (brute-force) reference dengan sklearn NearestNeighbors...\")\n","t0 = time.time()\n","nn_brute = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean').fit(X_scaled)\n","dist_ref, idx_ref = nn_brute.kneighbors(X_queries, return_distance=True)\n","t_brute_build = time.time() - t0\n","# query timing for brute force measured separately\n","tq0 = time.time()\n","dist_ref2, idx_ref2 = nn_brute.kneighbors(X_queries, return_distance=True)\n","t_brute_query = (time.time() - tq0) / n_queries\n","print(f\"Brute build time: {t_brute_build:.3f}s, avg query time: {t_brute_query*1000:.4f} ms/query\")\n","\n","# Utility: function to compute recall@k and avg query time\n","def evaluate_index(query_func, name):\n","    # query_func should accept (queries, k) and return (indices, distances) arrays\n","    gc.collect()\n","    t0 = time.time()\n","    inds, dists = query_func(X_queries, k)\n","    query_time = (time.time() - t0) / n_queries\n","    # compute recall@k: for each query, overlap between returned set and reference set\n","    correct = 0\n","    for i in range(len(inds)):\n","        set_ref = set(idx_ref[i])\n","        set_idx = set(inds[i])\n","        correct += len(set_ref.intersection(set_idx)) / float(k)\n","    recall_at_k = correct / len(inds)\n","    print(f\"{name}: avg query time = {query_time*1000:.4f} ms/query, recall@{k} = {recall_at_k:.4f}\")\n","    return {\"name\": name, \"avg_query_ms\": query_time*1000, \"recall_at_k\": recall_at_k}\n","\n","results = []\n","\n","# -----------------------\n","# 2) ANNOY\n","# -----------------------\n","try:\n","    print(\"\\n--- ANNOY ---\")\n","    from annoy import AnnoyIndex\n","    metric = 'euclidean'  # options: 'angular' 'euclidean' 'manhattan' 'hamming' 'dot'\n","    ann = AnnoyIndex(dim, metric)\n","    # build index\n","    t0 = time.time()\n","    for i, v in enumerate(X_scaled):\n","        ann.add_item(i, v)\n","    n_trees = 50   # tradeoff: lebih banyak trees -> lebih akurat tapi lebih lambat/dan memori\n","    ann.build(n_trees)\n","    t_build = time.time() - t0\n","    print(f\"Annoy build time: {t_build:.3f}s (n_trees={n_trees})\")\n","\n","    def annoy_query(queries, kk):\n","        inds = []\n","        dists = []\n","        for q in queries:\n","            ids, distances = ann.get_nns_by_vector(q.tolist(), kk, include_distances=True)\n","            inds.append(ids)\n","            dists.append(distances)\n","        return np.array(inds, dtype=int), np.array(dists, dtype=float)\n","\n","    res_annoy = evaluate_index(annoy_query, \"Annoy\")\n","    res_annoy[\"build_time_s\"] = t_build\n","    res_annoy[\"params\"] = {\"n_trees\": n_trees, \"metric\": metric}\n","    results.append(res_annoy)\n","except Exception as e:\n","    print(\"Error running Annoy:\", e)\n","\n","# -----------------------\n","# 3) FAISS\n","# -----------------------\n","try:\n","    print(\"\\n--- FAISS (CPU) ---\")\n","    import faiss\n","    # 3a) Exact Faiss (IndexFlatL2) for baseline exact (fast brute L2)\n","    t0 = time.time()\n","    index_flat = faiss.IndexFlatL2(dim)  # exact\n","    index_flat.add(X_scaled)\n","    t_build_flat = time.time() - t0\n","    print(f\"Faiss IndexFlatL2 build time: {t_build_flat:.3f}s\")\n","    # query function\n","    def faiss_flat_query(queries, kk):\n","        # faiss expects float32 numpy arrays\n","        D, I = index_flat.search(queries.astype('float32'), kk)\n","        return I, D\n","\n","    res_faiss_exact = evaluate_index(faiss_flat_query, \"Faiss(IndexFlatL2) exact\")\n","    res_faiss_exact[\"build_time_s\"] = t_build_flat\n","    results.append(res_faiss_exact)\n","\n","    # 3b) Approx Faiss: IVF + PQ or IVF Flat (IndexIVFFlat)\n","    nlist = 100  # number of clusters\n","    quantizer = faiss.IndexFlatL2(dim)\n","    index_ivf = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)\n","    # training\n","    t0 = time.time()\n","    # need training vectors: choose up to 100k randomly or all\n","    ntrain = min(100000, n_samples)\n","    train_idx = np.random.choice(n_samples, ntrain, replace=False)\n","    xb_train = X_scaled[train_idx]\n","    index_ivf.train(xb_train)\n","    index_ivf.add(X_scaled)\n","    t_build_ivf = time.time() - t0\n","    index_ivf.nprobe = 10  # tradeoff: larger -> more accurate slower\n","    print(f\"Faiss IndexIVFFlat build/train time: {t_build_ivf:.3f}s (nlist={nlist}, nprobe={index_ivf.nprobe})\")\n","\n","    def faiss_ivf_query(queries, kk):\n","        D, I = index_ivf.search(queries.astype('float32'), kk)\n","        return I, D\n","\n","    res_faiss_ivf = evaluate_index(faiss_ivf_query, \"Faiss(IndexIVFFlat) approx\")\n","    res_faiss_ivf[\"build_time_s\"] = t_build_ivf\n","    res_faiss_ivf[\"params\"] = {\"nlist\": nlist, \"nprobe\": index_ivf.nprobe}\n","    results.append(res_faiss_ivf)\n","\n","except Exception as e:\n","    print(\"Error running Faiss:\", e)\n","\n","# -----------------------\n","# 4) HNSWLIB\n","# -----------------------\n","try:\n","    print(\"\\n--- HNSWLIB ---\")\n","    import hnswlib\n","    p = hnswlib.Index(space='l2', dim=dim)  # use 'l2' metric for Euclidean\n","    M = 48          # tradeoff param (higher == more accuracy and memory)\n","    ef_construction = 200\n","    t0 = time.time()\n","    p.init_index(max_elements=n_samples, ef_construction=ef_construction, M=M)\n","    p.add_items(X_scaled, np.arange(n_samples))\n","    p.set_ef(50)   # query-time parameter: higher -> more accurate slower\n","    t_build_hnsw = time.time() - t0\n","    print(f\"HNSWLIB build time: {t_build_hnsw:.3f}s (M={M}, ef_construction={ef_construction}), ef_query={p.get_ef()}\")\n","\n","    def hnsw_query(queries, kk):\n","        labels, distances = p.knn_query(queries, k=kk)\n","        return labels, distances\n","\n","    res_hnsw = evaluate_index(hnsw_query, \"HNSWLIB\")\n","    res_hnsw[\"build_time_s\"] = t_build_hnsw\n","    res_hnsw[\"params\"] = {\"M\": M, \"ef_construction\": ef_construction, \"ef_query\": p.get_ef()}\n","    results.append(res_hnsw)\n","\n","except Exception as e:\n","    print(\"Error running hnswlib:\", e)\n","\n","# -----------------------\n","# Summary\n","# -----------------------\n","print(\"\\n=== SUMMARY ===\")\n","import json\n","print(json.dumps(results, indent=2))\n","\n","# Optionally save results\n","out_df = pd.DataFrame(results)\n","out_df.to_csv(\"ann_index_comparison_results.csv\", index=False)\n","print(\"Hasil disimpan ke ann_index_comparison_results.csv\")\n"]}]}